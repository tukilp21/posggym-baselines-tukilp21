import logging
logging.basicConfig(level=logging.DEBUG)
import math
import random
import time
from typing import Dict, Optional, Tuple, Union

import gymnasium as gym
import numpy as np
import posggym.model as M
import psutil
from posggym.agents.policy import Policy, PolicyState
from posggym.utils.history import JointHistory

import posggym_baselines.planning.belief as B
from posggym_baselines.planning.config import MCTSConfig
from posggym_baselines.planning.node import ActionNode, ObsNode
from posggym_baselines.planning.other_policy import OtherAgentPolicy
from posggym_baselines.planning.search_policy import SearchPolicy
from posggym_baselines.planning.utils import MinMaxStats, PlanningStatTracker


class MCTS:
    """Partially Observable Multi-Agent Monte-Carlo Planning.

    The is the base class for the various MCTS based algorithms.

    """

    def __init__(
        self,
        model: M.POSGModel,
        agent_id: str,
        config: MCTSConfig,
        other_agent_policies: Dict[str, OtherAgentPolicy],
        search_policy: SearchPolicy,
    ):
        self.model = model
        self.agent_id = agent_id
        self.config = config
        self.num_agents = len(model.possible_agents)
        assert len(other_agent_policies) == self.num_agents - 1
        assert agent_id not in other_agent_policies
        self.other_agent_policies = other_agent_policies
        self.search_policy = search_policy

        assert isinstance(model.action_spaces[agent_id], gym.spaces.Discrete)
        num_actions = model.action_spaces[agent_id].n
        self.action_space = list(range(num_actions))

        self._min_max_stats = MinMaxStats(config.known_bounds)
        self._rng = random.Random(config.seed)

        if config.step_limit is not None:
            self.step_limit = config.step_limit
        elif model.spec is not None:
            self.step_limit = model.spec.max_episode_steps
        else:
            self.step_limit = float("inf")

        self._reinvigorator = B.BeliefRejectionSampler(
            model,
            config.state_belief_only,
            sample_limit_factor=config.reinvigoration_sample_limit_factor,
        )

        if config.action_selection == "pucb":
            self._search_action_selection = self.pucb_action_selection
            self._final_action_selection = self.max_visit_action_selection
        elif config.action_selection == "ucb":
            self._search_action_selection = self.ucb_action_selection
            self._final_action_selection = self.max_value_action_selection
        else:
            self._search_action_selection = self.min_visit_action_selection
            self._final_action_selection = self.max_value_action_selection

        self.root = ObsNode(
            parent=None,
            obs=None,
            t=0,
            belief=B.ParticleBelief(self._rng),
            action_probs={None: 1.0},
            search_policy_state=self.search_policy.get_initial_state(),
        )
        self._last_action = None

        self._step_num = 0
        self.step_statistics: Dict[str, float] = {}
        self._reset_step_statistics()
        self.stat_tracker = PlanningStatTracker(self)

        self._logger = logging.getLogger()

    #######################################################
    # Step
    #######################################################

    def step(self, obs: M.ObsType) -> M.ActType:
        ''' main usage of the planner: taking a step ...'''

        assert self.root.t <= self.step_limit

        if self.root.is_absorbing:
            for k in self.step_statistics:
                self.step_statistics[k] = np.nan # set all elements to nan
            return self._last_action

        self._reset_step_statistics()

        self._log_info(f"Step {self._step_num} obs={obs}")
        self.update(self._last_action, obs)

        self._last_action = self.get_action()
        self._step_num += 1

        self.step_statistics["mem_usage"] = (
            psutil.Process().memory_info().rss / 1024**2
        )
        self.stat_tracker.step() # update and store statistics from planner.step_statistics

        return self._last_action

    #######################################################
    # RESET
    #######################################################

    def reset(self):
        self._log_info("Reset")
        self.stat_tracker.reset_episode()
        self._step_num = 0
        self._min_max_stats = MinMaxStats(self.config.known_bounds)
        self._reset_step_statistics()

        self.root = ObsNode(
            parent=None,
            obs=None,
            t=0,
            belief=B.ParticleBelief(self._rng),
            action_probs={None: 1.0},
            search_policy_state=self.search_policy.get_initial_state(),
        )
        self._last_action = None

    def _reset_step_statistics(self):
        self.step_statistics = {
            "search_time": 0.0,
            "update_time": 0.0,
            "reinvigoration_time": 0.0,
            "evaluation_time": 0.0,
            "policy_calls": 0,
            "inference_time": 0.0,
            "search_depth": 0,
            "num_sims": 0,
            "mem_usage": 0,
            "min_value": self._min_max_stats.minimum,
            "max_value": self._min_max_stats.maximum,
        }

    #######################################################
    # UPDATE
    #######################################################

    def update(self, action: M.ActType, obs: M.ObsType):
        self._log_info(f"Step {self.root.t} update for a={action} o={obs}")
        if self.root.is_absorbing:
            return

        start_time = time.time()
        if self.root.t == 0:
            self._last_action = None
            self._initial_update(obs)
        else:
            self._update(action, obs)

        update_time = time.time() - start_time
        self.step_statistics["update_time"] = update_time
        self._log_info(f"Update time = {update_time:.4f}s")

    def _initial_update(self, init_obs: M.ObsType):
        '''
        Initialise particle belief at root (current) node based on initial observation
            - case 1: model has get_agent_initial_belief implemented (by default is not)
            - case 2: only add belief particles that match initial observation (rejection sampling)
        '''
        action_node = self.root.add_child(None)
        obs_node = self._add_obs_node(action_node, init_obs, init_visits=0)

        try:
            # check if model has implemented get_agent_initial_belief
            self.model.sample_agent_initial_state(self.agent_id, init_obs)
            rejection_sample = False
        except NotImplementedError:
            rejection_sample = True

        hps_b0 = B.ParticleBelief(self._rng)
        init_actions = {i: None for i in self.model.possible_agents}
        while hps_b0.size() < self.config.num_particles + self.config.extra_particles:
            # do rejection sampling from initial belief with initial obs
            if rejection_sample:
                state = self.model.sample_initial_state()
                joint_obs = self.model.sample_initial_obs(state)
                if joint_obs[self.agent_id] != init_obs:
                    continue
            # 
            else:
                state = self.model.sample_agent_initial_state(self.agent_id, init_obs)
                joint_obs = self.model.sample_initial_obs(state)
                joint_obs[self.agent_id] = init_obs

            ###########################################
            # only track belief of a state - ex: POMCP
            if self.config.state_belief_only: 
                joint_history = None
                policy_state = None
            
            # otherwise track joint history and other agents policy states
            else:
                joint_history = JointHistory.get_init_history(
                    self.model.possible_agents, joint_obs
                ) # possible_agents return IDs of all possible agents that can interact with the environment
                policy_state = {
                    j: self.other_agent_policies[j].sample_initial_state()
                    for j in self.model.possible_agents
                    if j != self.agent_id
                }
                policy_state = self._update_other_agent_policies(
                    init_actions, joint_obs, policy_state
                )

            hps_b0.add_particle(
                B.HistoryPolicyState(
                    state,
                    joint_history,
                    policy_state,
                    t=1,
                )
            )

        obs_node.belief = hps_b0
        self.root = obs_node
        self.root.parent = None

    def _update(self, action: M.ActType, obs: M.ObsType):
        '''
        Update the root node based on the taken action and received observation. Also, pruning "dead" (previous) branches
        '''
        self._log_debug("Pruning histories")
        # Get root node's child
        try:
            action_node = self.root.get_child(action)
        except AssertionError as ex:
            if self.root.is_absorbing:
                action_node = self.root.add_child(action)
            else:
                raise ex

        try: # get child obs node matching given observation !!!

            obs_node = action_node.get_child(obs)
        except AssertionError:
            # Obs node not found
            # Add obs node with uniform policy prior
            # This will be updated in the course of doing simulations
            obs_node = self._add_obs_node(action_node, obs, init_visits=0)
            obs_node.is_absorbing = self.root.is_absorbing

        if obs_node.is_absorbing:
            self._log_debug("Absorbing state reached.")
        else:
            self._log_debug(
                f"Belief size before reinvigoration = {obs_node.belief.size()}"
            )
            ############ DEBUGGINGGGGGGGGGGGGGGGGGGGGGGGGGG
            tmp = obs_node.belief.size()
            self._log_debug(f"Parent belief size = {self.root.belief.size()}")
            self._reinvigorate(obs_node, action, obs)
            self._log_debug(
                f"Belief size after reinvigoration = {obs_node.belief.size()}"
            )
            # check if belief size changed after reinvigoration
            if obs_node.belief.size() != tmp:
                self._log_debug("Reinvigoration change belief size")
            if tmp == 0:
                self._log_debug("Belief size ZERO????")

        self.root = obs_node
        # remove reference to parent, effectively pruning dead branches
        obs_node.parent = None

    #######################################################
    # SEARCH
    #######################################################

    def get_action(self) -> M.ActType:
        '''  procedure SEARCH(h) in POMCP paper'''
        if self.root.is_absorbing:
            self._log_debug("Agent in absorbing state. Not running search.")
            return self.action_space[0]

        self._log_info(
            f"Searching for search_time_limit={self.config.search_time_limit}"
        )
        start_time = time.time()

        if len(self.root.children) == 0:
            # create action nodes for all possible actions, which will be visited during search
            for action in self.action_space:
                self.root.add_child(action)

        ###########################################################
        # Main algo loop
        max_search_depth = 0
        n_sims = 0

        #### debug !!!!!!!!!!!!!!! ####
        unique_states = {}
        for particle in self.root.belief.particles:
            # use robot state as key for counting unique states in belief
            robot_state = particle.state[0]
            unique_states[robot_state] = unique_states.get(robot_state, 0) + 1
        if len(unique_states) > 1:
            self._log_debug(f"Unique robot states in belief: {len(unique_states)}")
        #### debug !!!!!!!!!!!!!!! ####

        while time.time() - start_time < self.config.search_time_limit:
            hps = self.root.belief.sample()
            _, search_depth = self._simulate(hps, self.root, 0, self.search_policy)
        ########################################################
            self.root.visits += 1
            max_search_depth = max(max_search_depth, search_depth)
            n_sims += 1
        

        search_time = time.time() - start_time
        self.step_statistics["search_time"] = search_time
        self.step_statistics["search_depth"] = max_search_depth
        self.step_statistics["num_sims"] = n_sims
        self.step_statistics["min_value"] = self._min_max_stats.minimum
        self.step_statistics["max_value"] = self._min_max_stats.maximum
        self._log_info(f"{search_time=:.2f} {max_search_depth=}")
        if self.config.known_bounds is None:
            self._log_info(
                f"{self._min_max_stats.minimum=:.2f} "
                f"{self._min_max_stats.maximum=:.2f}"
            )
        self._log_info(f"Root node policy prior = {self.root.policy_str()}")

        return self._final_action_selection(self.root)

    def _simulate(
        self,
        hps: B.HistoryPolicyState,
        obs_node: ObsNode,
        depth: int,
        search_policy: Policy,
    ) -> Tuple[float, int]:
        '''
        SIMULATE(s, h, depth)
        - note that for POMCP, hps only contains ENV's state (s)
        - search_policy ~ rollout policy

        return R, depth
            - R here could be instant leaf node value, or rollout-ed / cumulated return
        '''

        if depth > self.config.depth_limit or obs_node.t > self.step_limit:
            return 0, depth


        ############# First time visiting this node?
        if len(obs_node.children) == 0: 
            # leaf node reached
            for action in self.action_space:
                obs_node.add_child(action)
            leaf_node_value = self._evaluate(
                hps,
                depth,
                search_policy,
                obs_node.search_policy_state,
            )
            return leaf_node_value, depth # backup

        ############ action selection - based on tree policy (e.g., UCT)
        ego_action = self._search_action_selection(obs_node) # tree policy
        joint_action = self._get_joint_action(hps, ego_action)

        ############ environment step (POMDP simulator)
        # - return (s', o, r) and done
        joint_step = self.model.step(hps.state, joint_action)
        joint_obs = joint_step.observations

        ego_obs = joint_obs[self.agent_id]
        ego_return = joint_step.rewards[self.agent_id]
        ego_done = (
            joint_step.terminations[self.agent_id]
            or joint_step.truncations[self.agent_id]
            or joint_step.all_done
        )

        if ego_done:
            # self._log_debug(f"Terminal state reached at depth {depth} with return {ego_return:.2f}")
            pass

        # create child_obs_node with
        # - set of particle belief
        if self.config.state_belief_only: # ex: POMCP
            new_history = None
            next_pi_state = None
        else:
            new_history = hps.history.extend(joint_action, joint_obs)
            next_pi_state = self._update_other_agent_policies(
                joint_action, joint_obs, hps.policy_state
            )
            
        next_hps = B.HistoryPolicyState(
            joint_step.state, new_history, next_pi_state, hps.t + 1
        )
        
        action_node = obs_node.get_child(ego_action)

        # Check if action node (next node) has a child node matching history - if yes, then add new particle to exisit
        if action_node.has_child(ego_obs):
            child_obs_node = action_node.get_child(ego_obs)
            child_obs_node.visits += 1
            # Add search policy distribution to moving average policy of node
            action_probs = search_policy.get_pi(child_obs_node.search_policy_state)
            for a, a_prob in action_probs.items():
                old_prob = child_obs_node.action_probs[a]
                child_obs_node.action_probs[a] += (
                    a_prob - old_prob
                ) / child_obs_node.visits
        else:
            child_obs_node = self._add_obs_node(action_node, ego_obs, init_visits=1)
        
        child_obs_node.is_absorbing = ego_done
        child_obs_node.belief.add_particle(next_hps)

        
        ####### recursive simulation 
        max_depth = depth
        if not ego_done:
            future_return, max_depth = self._simulate(
                next_hps, child_obs_node, depth + 1, search_policy
            )
            ego_return += self.config.discount * future_return

        ###### BACKUP
        action_node.update(ego_return)
        self._min_max_stats.update(action_node.value)
        return ego_return, max_depth

    def _evaluate(
        self,
        hps: B.HistoryPolicyState,
        depth: int,
        rollout_policy: Policy,
        rollout_policy_state: PolicyState,
    ) -> float:
        ''' Wrapper for truncated rollout's strategy (if applicable)'''

        start_time = time.time()
        if self.config.truncated:
            try:
                '''instead of simulating all the way to episode end, use a learned value estimator to terminate early.'''
                v = rollout_policy.get_value(rollout_policy_state)
            except NotImplementedError as e:
                if self.config.use_rollout_if_no_value:
                    v = self._rollout(hps, depth, rollout_policy, rollout_policy_state)
                else:
                    raise e
                
        else: ##########: using this for POMCP
            v = self._rollout(hps, depth, rollout_policy, rollout_policy_state)

        self.step_statistics["evaluation_time"] += time.time() - start_time

        return v

    def _rollout(
        self,
        hps: B.HistoryPolicyState,
        depth: int,
        rollout_policy: Policy,
        rollout_policy_state: PolicyState,
    ) -> float:
        ''' A loop (instead of recursion) to get cumulative return until termination or depth limit '''

        agent_return = 0
        rollout_t = 0
        while depth <= self.config.depth_limit and hps.t <= self.step_limit:
            ############ action selection
            ego_action = rollout_policy.sample_action(rollout_policy_state)
            joint_action = self._get_joint_action(hps, ego_action)

            ############ environment step (POMDP simulator)
            # - return (s', o, r) and done
            joint_step = self.model.step(hps.state, joint_action)
            joint_obs = joint_step.observations

            ########### MAIN: Compute cumulative R
            agent_return += (
                self.config.discount**rollout_t * joint_step.rewards[self.agent_id]
            )

            # -------- UPDATE --------------------------------
            # termination check
            if (joint_step.terminations[self.agent_id]
                or joint_step.truncations[self.agent_id]
                or joint_step.all_done):
                break

            if self.config.state_belief_only:
                new_history = None
                next_pi_state = None
            else:
                new_history = hps.history.extend(joint_action, joint_obs)
                next_pi_state = self._update_other_agent_policies(
                    joint_action, joint_obs, hps.policy_state
                )
            hps = B.HistoryPolicyState(
                joint_step.state, new_history, next_pi_state, hps.t + 1
            )
            rollout_policy_state = self._update_policy_state(
                joint_action[self.agent_id],
                joint_obs[self.agent_id],
                rollout_policy,
                rollout_policy_state,
            )

            depth += 1
            rollout_t += 1

        return agent_return

    def _update_policy_state(
        self,
        action: M.ActType,
        obs: M.ObsType,
        policy: Union[Policy, OtherAgentPolicy],
        policy_state: PolicyState,
    ) -> PolicyState:
        '''
        this is just a wrapper around policy.get_next_state but also keeps track of
        inference time and number of policy calls
        '''
        start_time = time.time()
        next_hidden_state = policy.get_next_state(action, obs, policy_state)
        self.step_statistics["inference_time"] += time.time() - start_time
        self.step_statistics["policy_calls"] += 1
        return next_hidden_state

    def _update_other_agent_policies(
        self,
        joint_action: Dict[str, Optional[M.ActType]],
        joint_obs: Dict[str, M.ObsType],
        pi_state: Dict[str, PolicyState],
    ) -> Dict[str, PolicyState]:
        next_policy_state = {}
        for i in self.model.possible_agents:
            if i == self.agent_id or i not in joint_action:
                continue

            next_policy_state[i] = self._update_policy_state(
                joint_action[i],
                joint_obs[i],
                self.other_agent_policies[i],
                pi_state[i],
            )
        return next_policy_state

    #######################################################
    # ACTION SELECTION a.k.a Tree policy
    #######################################################

    def pucb_action_selection(self, obs_node: ObsNode) -> M.ActType:
        """Select action from node using PUCB."""
        if obs_node.visits == 0:
            # sample action using prior policy
            return random.choices(
                list(obs_node.action_probs.keys()),
                weights=list(obs_node.action_probs.values()),
                k=1,
            )[0]

        # add exploration noise to prior
        prior, noise = {}, 1 / len(self.action_space)
        for a in self.action_space:
            prior[a] = (
                obs_node.action_probs[a] * (1 - self.config.pucb_exploration_fraction)
                + self.config.pucb_exploration_fraction * noise
            )

        sqrt_n = math.sqrt(obs_node.visits)
        max_v = -float("inf")
        max_action = 0
        for action_node in obs_node.get_child_nodes():
            explore_v = (
                self.config.c
                * prior[action_node.action]
                * (sqrt_n / (1 + action_node.visits))
            )
            if action_node.visits > 0:
                action_v = self._min_max_stats.normalize(action_node.value)
            else:
                action_v = 0
            action_v = action_v + explore_v
            if action_v > max_v:
                max_v = action_v
                max_action = action_node.action
        return max_action

    def ucb_action_selection(self, obs_node: ObsNode) -> M.ActType:
        """Select action from node using UCB."""
        if obs_node.visits == 0:
            return random.choice(self.action_space)

        log_n = math.log(obs_node.visits)

        max_v = -float("inf")
        max_action = 0
        for action_node in obs_node.get_child_nodes():
            if action_node.visits == 0:
                return action_node.action
            explore_v = self.config.c * math.sqrt(log_n / action_node.visits)
            action_v = self._min_max_stats.normalize(action_node.value) + explore_v
            if action_v > max_v:
                max_v = action_v
                max_action = action_node.action
        return max_action

    def min_visit_action_selection(self, obs_node: ObsNode) -> M.ActType:
        """Select action from node with least visits.

        Note this guarantees all actions are visited equally +/- 1 when used
        during search.
        """
        if obs_node.visits == 0:
            return random.choice(self.action_space)

        min_n = obs_node.visits + 1
        next_action = 0
        for action_node in obs_node.get_child_nodes():
            if action_node.visits < min_n:
                min_n = action_node.visits
                next_action = action_node.action
        return next_action

    def max_visit_action_selection(self, obs_node: ObsNode) -> M.ActType:
        """Select action from node with most visits.

        Breaks ties randomly.
        """
        if obs_node.visits == 0:
            return random.choice(self.action_space)

        max_actions = []
        max_visits = 0
        for action_node in obs_node.get_child_nodes():
            if action_node.visits == max_visits:
                max_actions.append(action_node.action)
            elif action_node.visits > max_visits:
                max_visits = action_node.visits
                max_actions = [action_node.action]
        return random.choice(max_actions)

    def max_value_action_selection(self, obs_node: ObsNode) -> M.ActType:
        """Select action from node with maximum value.

        Breaks ties randomly.
        """
        if len(obs_node.children) == 0:
            # Node not expanded so select random action
            return random.choice(self.action_space)

        max_actions = []
        max_value = -float("inf")
        for action_node in obs_node.get_child_nodes():
            if action_node.value == max_value:
                max_actions.append(action_node.action)
            elif action_node.value > max_value:
                max_value = action_node.value
                max_actions = [action_node.action]
        return random.choice(max_actions)

    def _get_joint_action(
        self, hps: B.HistoryPolicyState, ego_action: M.ActType
    ) -> Dict[str, M.ActType]:
        ''' Get joint action for all agents given ego action and other agents policies. '''    

        agent_actions = {}
        for i in self.model.possible_agents:
            if i == self.agent_id:
                a_i = ego_action
            elif self.config.state_belief_only:
                # assume other agents policies are stateless, i.e. Random
                a_i = self.other_agent_policies[i].sample_action({})
            else:
                a_i = self.other_agent_policies[i].sample_action(hps.policy_state[i])
            agent_actions[i] = a_i
        return agent_actions

    #######################################################
    # GENERAL METHODS
    #######################################################

    def _add_obs_node(
        self,
        parent: ActionNode,
        obs: M.ObsType,
        init_visits: int = 0,
    ) -> ObsNode:
        next_search_policy_state = self._update_policy_state(
            parent.action,
            obs,
            self.search_policy,
            parent.parent.search_policy_state,
        )

        obs_node = ObsNode(
            parent,
            obs,
            t=parent.t + 1,
            belief=B.ParticleBelief(self._rng),
            action_probs=self.search_policy.get_pi(next_search_policy_state),
            search_policy_state=next_search_policy_state,
            init_value=0.0,
            init_visits=init_visits,
        )
        parent.add_child_node(obs_node)
        return obs_node

    #######################################################
    # BELIEF REINVIGORATION
    #######################################################

    def _reinvigorate(
        self,
        obs_node: ObsNode,
        action: M.ActType,
        obs: M.ObsType,
        target_node_size: Optional[int] = None,
    ):
        """Reinvigoration belief associated to given history.

        The general reinvigoration process:
        1. Check if belief needs to be reinvigorated (e.g. it's not a root belief)
        2. Reinvigorate node using rejection sampling/custom function for fixed number of tries
        3. If desired number of particles not sampled using rejection sampling/custom function then sample remaining particles using sampling without rejection
        """
        start_time = time.time()

        belief_size = obs_node.belief.size()
        if belief_size is None:
            # root belief
            return

        if target_node_size is None:
            #################################################
            # NOTE: this is K in the paper
            particles_to_add = self.config.num_particles + self.config.extra_particles
        else:
            particles_to_add = target_node_size
        particles_to_add -= belief_size

        if particles_to_add <= 0:
            return

        parent_obs_node = obs_node.parent.parent
        assert parent_obs_node is not None

        self._reinvigorator.reinvigorate(
            self.agent_id,
            obs_node.belief,
            action,
            obs,
            num_particles=particles_to_add,
            parent_belief=parent_obs_node.belief,
            joint_action_fn=self._reinvigorate_action_fn,
            joint_update_fn=self._reinvigorate_update_fn,
            **{"use_rejected_samples": True},  # used for rejection sampling
        )

        reinvig_time = time.time() - start_time
        self.step_statistics["reinvigoration_time"] += reinvig_time

    def _reinvigorate_action_fn(
        self, hps: B.HistoryPolicyState, ego_action: M.ActType
    ) -> Dict[str, M.ActType]:
        return self._get_joint_action(hps, ego_action)

    def _reinvigorate_update_fn(
        self,
        hps: B.HistoryPolicyState,
        joint_action: Dict[str, M.ActType],
        joint_obs: Dict[str, M.ObsType],
    ) -> Dict[str, PolicyState]:
        return self._update_other_agent_policies(
            joint_action, joint_obs, hps.policy_state
        )

    #######################################################
    # Logging and General methods
    #######################################################

    def close(self):
        """Do any clean-up."""
        self.search_policy.close()
        for policy in self.other_agent_policies.values():
            policy.close()

    def _log_info(self, msg: str):
        """Log an info message."""
        self._logger.log(logging.INFO - 1, self._format_msg(msg))

    def _log_debug(self, msg: str):
        """Log a debug message."""
        self._logger.debug(self._format_msg(msg))

    def _format_msg(self, msg: str):
        return f"i={self.agent_id} {msg}"

    def __str__(self):
        return f"{self.__class__.__name__}"

